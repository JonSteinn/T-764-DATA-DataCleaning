@article{lev,
  abstract = {Seems to be the first person to define what became  known as the (simple) edit-distance and show it to be a  metric. Paper mostly about constructing optimal codes  to transmit such corrections.},
  added-at = {2009-09-11T19:06:23.000+0200},
  author = {Levenshtein, Vladimir Iosifovich},
  biburl = {https://www.bibsonomy.org/bibtex/220546d80ce76f58c6ef6ece9dd5f5056/jimregan},
  interhash = {55f7ad93fcb9ae3ed999afaa6e24937d},
  intrahash = {20546d80ce76f58c6ef6ece9dd5f5056},
  journal = {Soviet Physics Doklady},
  keywords = {edit-distance},
  month = {2},
  note = {Doklady Akademii Nauk SSSR, V163 No4 845-848 1965},
  number = 8,
  pages = {707--710},
  timestamp = {2009-09-11T19:06:23.000+0200},
  title = {Binary codes capable of correcting deletions, insertions and reversals.},
  volume = 10,
  year = 1966
}
@book{berman,
author = {Berman, Jules J.},
title = {Principles of Big Data: Preparing, Sharing, and Analyzing Complex Information},
year = {2013},
isbn = {9780124045767},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Principles of Big Data helps readers avoid the common mistakes that endanger all Big Data projects. By stressing simple, fundamental concepts, this book teaches readers how to organize large volumes of complex data, and how to achieve data permanence when the content of the data is constantly changing. General methods for data verification and validation, as specifically applied to Big Data resources, are stressed throughout the book. The book demonstrates how adept analysts can find relationships among data objects held in disparate Big Data resources, when the data objects are endowed with semantic support (i.e., organized in classes of uniquely identified data objects). Readers will learn how their data can be integrated with data from other resources, and how the data extracted from Big Data resources can be used for purposes beyond those imagined by the data creators. Learn general methods for specifying Big Data in a way that is understandable to humans and to computers. Avoid the pitfalls in Big Data design and analysis. Understand how to create and use Big Data safely and responsibly with a set of laws, regulations and ethical standards that apply to the acquisition, distribution and integration of Big Data resources. Table of Contents Preface Introduction 1. Big Data Moves to the Center of the Universe 2. Measurement 3. Annotation 4. Identification, De-identification, and Re-identification 5. Ontologies and Semantics: How information is endowed with meaning 6. Standards and their Versions 7. Legacy Data 8. Hypothesis Testing 9. Prediction 10. Software 11. Complexity 12. Vulnerabilities 13. Legalities 14. Social and Ethical Issues}
}