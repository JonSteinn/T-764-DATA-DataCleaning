The best way to avoid duplicates is not to produce them to begin with. That can be easier said than done as we might have limited control over data in the wilderness. Redundant data increases as data grows and given its volume today, the need for data cleaning has never been greater.

The legacy database we look at is from an Icelandic volleyball association where records were manually entered. Existing individuals were looked up by name and birthday to determine if they already existed. If not a new individual was created. 

Even in a country as sparsely populated as Iceland and its small subset of people associated with volleyball one cannot assume uniqueness of names. Adding date of birth might suffice (for this scenario, not generally) but one never knows beforehand and there are so many issues associated with such identifiers. Berman \cite{berman} goes over a few such as that names can change and special typographic characters and probably the most common one, the human error factor.

We apply similarity matching and domain specific string manipulation to try to detect as many duplicates as possible with the aim of minimizing false positives.

The rest of this paper is structured as follows. Chapter \ref{ch:backgr} defines our similarity measure. In chapters \ref{ch:methods} and \ref{ch:res} we describe our implementation and experimental setup. Potential improvements are listed in Chapter \ref{ch:futurework} and finally we interpret the results in Chapter \ref{ch:conclusion}.
